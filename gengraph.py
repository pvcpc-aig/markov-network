"""
Using the concatenated news articles resulting from `merge.py`, this
script will take the words in all articles and build a markov chain
graph and output it to a file for later use.

The markov chain is generated by taking a token and creating a forward
edge to the token that follows immediately. During the post-processing
stage, the probabilities are computed for each edge.
"""
import os
import io
import sys

from nltk.tokenize import word_tokenize


# The concatenated articles file with only the content.
ARTICLES = "data/allthenews/articles.csv"

# An 8MB read buffer.
READ_BUFFER = 2 ** 23

# An 8MB write buffer.
WRITE_BUFFER = 2 ** 23

# The file where the table will be output
OUT_TABLE = "data/allthenews/markov"


if __name__ == "__main__":
    # The graph is represented by a dictionary mapping each unique token
    # to a map of succeeding tokens and their frequencies after the
    # said token. The graph is then preprocessed to compute probabilities
    # and then the graph is output to a file in the following format:
    #
    #   <T> <X_0> <C_0> <X_1> <C_1> ... <X_n> <C_n>
    #
    # where T is any unique token and the sequence of X are the indices pointing
    # to other tokens in the table. The indices correspond to line numbers
    # in the table file. The sequence C contains the probabilities to the
    # corresponding token indices sorted from greatest to least.
    graph = dict()

    print("Generating graph...")
    duplicates_allowed = [ "had" ]
    with open(ARTICLES, mode="r", buffering=READ_BUFFER) as fin:
        line = 0
        for article in fin:
            line += 1
            # Use split instead of word tokenize for better quality
            # tokens.
            tokens = article.split() 
            if len(tokens) <= 1:
                continue
            i = 0
            while i < len(tokens):
                present = tokens[i]
                if present not in graph:
                    graph[present] = dict()
                if i == len(tokens) - 1: # the last token in the content.
                    break
                future = tokens[i + 1]
                i += 1
                if len(present) == 0 or \
                   len(future)  == 0:
                   continue # ignore empty tokens just in case.
                edges = graph[present]
                if future in edges:
                    edges[future] += 1
                else:
                    edges[future] = 1
    
    print("Computing probabilities...")
    for token, mapping in graph.items():
        total_freq = 0
        for c in mapping.values():
            total_freq += c
        if total_freq == 0:
            continue
        for x in mapping.keys():
            mapping[x] /= total_freq
    
    print("Generating table...")
    table = []
    idx_map = dict()
    for i, token in enumerate(graph.keys()):
        table.append([ token, [], [] ])
        idx_map[token] = i
    for token, mapping in graph.items():
        idx = idx_map[token]
        _, Xs, Cs = table[idx]
        for successor, prob in sorted(mapping.items(), key=lambda x: x[1], reverse=True):
            Xs.append(idx_map[successor])
            Cs.append(prob)
    
    print("Writing table...")
    with open(OUT_TABLE, mode="w", buffering=WRITE_BUFFER) as fout:
        for token, Xs, Cs in table:
            fout.write(f"{token} ")
            for x, c in zip(Xs, Cs):
                fout.write(f"{x} {c} ")
            fout.write('\n')
    
    print("Done...")